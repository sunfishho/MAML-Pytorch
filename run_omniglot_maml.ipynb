{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5adbaa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchonn as onn\n",
    "from torchonn.models import ONNBaseModel\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.stats as stats\n",
    "from copy import deepcopy\n",
    "from PIL import Image, ImageFilter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24e10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotNShot:\n",
    "\n",
    "    def __init__(self, root, batchsz, n_way, k_shot, k_query, imgsz):\n",
    "        \"\"\"\n",
    "        Different from mnistNShot, the\n",
    "        :param root:\n",
    "        :param batchsz: task num\n",
    "        :param n_way:\n",
    "        :param k_shot:\n",
    "        :param k_qry:\n",
    "        :param imgsz:\n",
    "        \"\"\"\n",
    "\n",
    "        self.resize = imgsz\n",
    "        if not os.path.isfile(os.path.join(root, 'omniglot.npy')):\n",
    "            # if root/data.npy does not exist, just download it\n",
    "            self.x = Omniglot(root, download=True,\n",
    "                              transform=transforms.Compose([lambda x: Image.open(x).convert('L'),\n",
    "                                                            lambda x: x.resize((imgsz, imgsz)),\n",
    "                                                            lambda x: np.reshape(x, (imgsz, imgsz, 1)),\n",
    "                                                            lambda x: np.transpose(x, [2, 0, 1]),\n",
    "                                                            lambda x: x/255.])\n",
    "                              )\n",
    "\n",
    "            temp = dict()  # {label:img1, img2..., 20 imgs, label2: img1, img2,... in total, 1623 label}\n",
    "            for (img, label) in self.x:\n",
    "                if label in temp.keys():\n",
    "                    temp[label].append(img)\n",
    "                else:\n",
    "                    temp[label] = [img]\n",
    "\n",
    "            self.x = []\n",
    "            for label, imgs in temp.items():  # labels info deserted , each label contains 20imgs\n",
    "                self.x.append(np.array(imgs))\n",
    "\n",
    "            # as different class may have different number of imgs\n",
    "            self.x = np.array(self.x).astype(np.float)  # [[20 imgs],..., 1623 classes in total]\n",
    "            # each character contains 20 imgs\n",
    "            print('data shape:', self.x.shape)  # [1623, 20, 84, 84, 1]\n",
    "            temp = []  # Free memory\n",
    "            # save all dataset into npy file.\n",
    "            np.save(os.path.join(root, 'omniglot.npy'), self.x)\n",
    "            print('write into omniglot.npy.')\n",
    "        else:\n",
    "            # if data.npy exists, just load it.\n",
    "            self.x = np.load(os.path.join(root, 'omniglot.npy'))\n",
    "            print('load from omniglot.npy.')\n",
    "\n",
    "        # [1623, 20, 84, 84, 1]\n",
    "        # TODO: can not shuffle here, we must keep training and test set distinct!\n",
    "        self.x_train, self.x_test = self.x[:1200], self.x[1200:]\n",
    "\n",
    "        # self.normalization()\n",
    "\n",
    "        self.batchsz = batchsz\n",
    "        self.n_cls = self.x.shape[0]  # 1623\n",
    "        self.n_way = n_way  # n way\n",
    "        self.k_shot = k_shot  # k shot\n",
    "        self.k_query = k_query  # k query\n",
    "        assert (k_shot + k_query) <=20\n",
    "\n",
    "        # save pointer of current read batch in total cache\n",
    "        self.indexes = {\"train\": 0, \"test\": 0}\n",
    "        self.datasets = {\"train\": self.x_train, \"test\": self.x_test}  # original data cached\n",
    "        print(\"DB: train\", self.x_train.shape, \"test\", self.x_test.shape)\n",
    "\n",
    "        self.datasets_cache = {\"train\": self.load_data_cache(self.datasets[\"train\"]),  # current epoch data cached\n",
    "                               \"test\": self.load_data_cache(self.datasets[\"test\"])}\n",
    "\n",
    "    def normalization(self):\n",
    "        \"\"\"\n",
    "        Normalizes our data, to have a mean of 0 and sdt of 1\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "        # print(\"before norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "        self.x_train = (self.x_train - self.mean) / self.std\n",
    "        self.x_test = (self.x_test - self.mean) / self.std\n",
    "\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "\n",
    "    # print(\"after norm:\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "\n",
    "    def load_data_cache(self, data_pack):\n",
    "        \"\"\"\n",
    "        Collects several batches data for N-shot learning\n",
    "        :param data_pack: [cls_num, 20, 84, 84, 1]\n",
    "        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "        \"\"\"\n",
    "        #  take 5 way 1 shot as example: 5 * 1\n",
    "        setsz = self.k_shot * self.n_way\n",
    "        querysz = self.k_query * self.n_way\n",
    "        data_cache = []\n",
    "\n",
    "        # print('preload next 50 caches of batchsz of batch.')\n",
    "        for sample in range(10):  # num of episodes\n",
    "\n",
    "            x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
    "            for i in range(self.batchsz):  # one batch means one set\n",
    "\n",
    "                x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
    "                selected_cls = np.random.choice(data_pack.shape[0], self.n_way, False)\n",
    "\n",
    "                for j, cur_class in enumerate(selected_cls):\n",
    "\n",
    "                    selected_img = np.random.choice(20, self.k_shot + self.k_query, False)\n",
    "\n",
    "                    # meta-training and meta-test\n",
    "                    x_spt.append(data_pack[cur_class][selected_img[:self.k_shot]])\n",
    "                    x_qry.append(data_pack[cur_class][selected_img[self.k_shot:]])\n",
    "                    y_spt.append([j for _ in range(self.k_shot)])\n",
    "                    y_qry.append([j for _ in range(self.k_query)])\n",
    "\n",
    "                # shuffle inside a batch\n",
    "                perm = np.random.permutation(self.n_way * self.k_shot)\n",
    "                x_spt = np.array(x_spt).reshape(self.n_way * self.k_shot, 1, self.resize, self.resize)[perm]\n",
    "                y_spt = np.array(y_spt).reshape(self.n_way * self.k_shot)[perm]\n",
    "                perm = np.random.permutation(self.n_way * self.k_query)\n",
    "                x_qry = np.array(x_qry).reshape(self.n_way * self.k_query, 1, self.resize, self.resize)[perm]\n",
    "                y_qry = np.array(y_qry).reshape(self.n_way * self.k_query)[perm]\n",
    "\n",
    "                # append [sptsz, 1, 84, 84] => [b, setsz, 1, 84, 84]\n",
    "                x_spts.append(x_spt)\n",
    "                y_spts.append(y_spt)\n",
    "                x_qrys.append(x_qry)\n",
    "                y_qrys.append(y_qry)\n",
    "\n",
    "\n",
    "            # [b, setsz, 1, 84, 84]\n",
    "            x_spts = np.array(x_spts).astype(np.float32).reshape(self.batchsz, setsz, 1, self.resize, self.resize)\n",
    "            y_spts = np.array(y_spts).astype(np.int32).reshape(self.batchsz, setsz)\n",
    "            # [b, qrysz, 1, 84, 84]\n",
    "            x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n",
    "            y_qrys = np.array(y_qrys).astype(np.int32).reshape(self.batchsz, querysz)\n",
    "\n",
    "            data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "\n",
    "        return data_cache\n",
    "\n",
    "    def next(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Gets next batch from the dataset with name.\n",
    "        :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # update cache if indexes is larger cached num\n",
    "        if self.indexes[mode] >= len(self.datasets_cache[mode]):\n",
    "            self.indexes[mode] = 0\n",
    "            self.datasets_cache[mode] = self.load_data_cache(self.datasets[mode])\n",
    "\n",
    "        next_batch = self.datasets_cache[mode][self.indexes[mode]]\n",
    "        self.indexes[mode] += 1\n",
    "\n",
    "        return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba7bc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from omniglot.npy.\n",
      "DB: train (1200, 20, 1, 28, 28) test (423, 20, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "db_train = OmniglotNShot('omniglot',\n",
    "                       batchsz=32,\n",
    "                       n_way=5,\n",
    "                       k_shot=12,\n",
    "                       k_query=8,\n",
    "                       imgsz=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb956ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "x_spt_ua = torch.from_numpy(np.array([0]))\n",
    "y_spt_ua = torch.from_numpy(np.array([0]))\n",
    "x_qry_ua = torch.from_numpy(np.array([0]))\n",
    "y_qry_ua = torch.from_numpy(np.array([0]))\n",
    "\n",
    "x_spt, y_spt, x_qry, y_qry = db_train.next()\n",
    "x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), torch.from_numpy(y_spt).to(device), \\\n",
    "                             torch.from_numpy(x_qry).to(device), torch.from_numpy(y_qry).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ad56f7-a9cc-41dc-855c-e23d7a889649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=40000, n_way=5, k_spt=1, k_qry=15, imgsz=28, imgc=1, task_num=32, meta_lr=0.001, update_lr=0.4, update_step=5, update_step_test=10)\n",
      "Meta(\n",
      "  (net): Learner(\n",
      "    conv2d:(ch_in:1, ch_out:64, k:3x3, stride:2, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(64,)\n",
      "    conv2d:(ch_in:64, ch_out:64, k:3x3, stride:2, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(64,)\n",
      "    conv2d:(ch_in:64, ch_out:64, k:3x3, stride:2, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(64,)\n",
      "    conv2d:(ch_in:64, ch_out:64, k:2x2, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(64,)\n",
      "    flatten:()\n",
      "    linear:(in:64, out:5)\n",
      "    \n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 64x1x3x3]\n",
      "        (1): Parameter containing: [torch.float32 of size 64]\n",
      "        (2): Parameter containing: [torch.float32 of size 64]\n",
      "        (3): Parameter containing: [torch.float32 of size 64]\n",
      "        (4): Parameter containing: [torch.float32 of size 64x64x3x3]\n",
      "        (5): Parameter containing: [torch.float32 of size 64]\n",
      "        (6): Parameter containing: [torch.float32 of size 64]\n",
      "        (7): Parameter containing: [torch.float32 of size 64]\n",
      "        (8): Parameter containing: [torch.float32 of size 64x64x3x3]\n",
      "        (9): Parameter containing: [torch.float32 of size 64]\n",
      "        (10): Parameter containing: [torch.float32 of size 64]\n",
      "        (11): Parameter containing: [torch.float32 of size 64]\n",
      "        (12): Parameter containing: [torch.float32 of size 64x64x2x2]\n",
      "        (13): Parameter containing: [torch.float32 of size 64]\n",
      "        (14): Parameter containing: [torch.float32 of size 64]\n",
      "        (15): Parameter containing: [torch.float32 of size 64]\n",
      "        (16): Parameter containing: [torch.float32 of size 5x64]\n",
      "        (17): Parameter containing: [torch.float32 of size 5]\n",
      "    )\n",
      "    (vars_bn): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 64]\n",
      "        (1): Parameter containing: [torch.float32 of size 64]\n",
      "        (2): Parameter containing: [torch.float32 of size 64]\n",
      "        (3): Parameter containing: [torch.float32 of size 64]\n",
      "        (4): Parameter containing: [torch.float32 of size 64]\n",
      "        (5): Parameter containing: [torch.float32 of size 64]\n",
      "        (6): Parameter containing: [torch.float32 of size 64]\n",
      "        (7): Parameter containing: [torch.float32 of size 64]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 91781\n",
      "load from omniglot.npy.\n",
      "DB: train (1200, 20, 1, 28, 28) test (423, 20, 1, 28, 28)\n",
      "step: 0 \ttraining acc: [0.19208333 0.2975     0.36208333 0.3775     0.37833333 0.38041667]\n",
      "val_accs: [0.3804166666666667]\n",
      "Test acc: [0.2002 0.3245 0.411  0.4248 0.4265 0.4272 0.428  0.429  0.4292 0.43\n",
      " 0.4302]\n",
      "test_accs_list: [0.4302]\n",
      "step: 50 \ttraining acc: [0.205      0.54791667 0.59166667 0.6175     0.61583333 0.61625   ]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625]\n",
      "step: 100 \ttraining acc: [0.20125    0.63291667 0.66208333 0.66958333 0.66958333 0.67083333]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333]\n",
      "step: 150 \ttraining acc: [0.21916667 0.71541667 0.75541667 0.76208333 0.7625     0.76416667]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667]\n",
      "step: 200 \ttraining acc: [0.1975     0.80958333 0.8275     0.82958333 0.83083333 0.83333333]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 250 \ttraining acc: [0.19083333 0.805      0.82458333 0.82375    0.82791667 0.8275    ]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334, 0.7779166666666667, 0.7904166666666667, 0.84, 0.8120833333333334, 0.8470833333333333, 0.8454166666666667, 0.82875, 0.8070833333333334, 0.8283333333333334, 0.83375, 0.8108333333333333, 0.8358333333333333, 0.8333333333333334, 0.8166666666666667, 0.84625, 0.83, 0.84125, 0.8308333333333333, 0.8283333333333334, 0.8283333333333334, 0.8354166666666667, 0.83, 0.83875, 0.8625, 0.82125, 0.8195833333333333, 0.84, 0.8379166666666666, 0.8283333333333334, 0.8233333333333334, 0.85125, 0.8445833333333334, 0.8520833333333333, 0.8429166666666666, 0.81875, 0.8295833333333333, 0.84125, 0.8508333333333333, 0.84125, 0.84125, 0.8495833333333334, 0.8375, 0.8154166666666667, 0.8554166666666667, 0.8566666666666667, 0.8645833333333334, 0.8233333333333334, 0.86375, 0.8466666666666667, 0.8275]\n",
      "step: 300 \ttraining acc: [0.175      0.845      0.86083333 0.86       0.86083333 0.86125   ]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334, 0.7779166666666667, 0.7904166666666667, 0.84, 0.8120833333333334, 0.8470833333333333, 0.8454166666666667, 0.82875, 0.8070833333333334, 0.8283333333333334, 0.83375, 0.8108333333333333, 0.8358333333333333, 0.8333333333333334, 0.8166666666666667, 0.84625, 0.83, 0.84125, 0.8308333333333333, 0.8283333333333334, 0.8283333333333334, 0.8354166666666667, 0.83, 0.83875, 0.8625, 0.82125, 0.8195833333333333, 0.84, 0.8379166666666666, 0.8283333333333334, 0.8233333333333334, 0.85125, 0.8445833333333334, 0.8520833333333333, 0.8429166666666666, 0.81875, 0.8295833333333333, 0.84125, 0.8508333333333333, 0.84125, 0.84125, 0.8495833333333334, 0.8375, 0.8154166666666667, 0.8554166666666667, 0.8566666666666667, 0.8645833333333334, 0.8233333333333334, 0.86375, 0.8466666666666667, 0.8275, 0.8633333333333333, 0.8558333333333333, 0.8591666666666666, 0.8475, 0.8229166666666666, 0.84375, 0.84, 0.85375, 0.8529166666666667, 0.8491666666666666, 0.8870833333333333, 0.8495833333333334, 0.8233333333333334, 0.8491666666666666, 0.8708333333333333, 0.8591666666666666, 0.8541666666666666, 0.8458333333333333, 0.8679166666666667, 0.84375, 0.8579166666666667, 0.8625, 0.8408333333333333, 0.8816666666666667, 0.85625, 0.8483333333333334, 0.8220833333333334, 0.8595833333333334, 0.8729166666666667, 0.8554166666666667, 0.89125, 0.86125, 0.8554166666666667, 0.8575, 0.8904166666666666, 0.8595833333333334, 0.8233333333333334, 0.8558333333333333, 0.8554166666666667, 0.8825, 0.8633333333333333, 0.8808333333333334, 0.90125, 0.8820833333333333, 0.84875, 0.8708333333333333, 0.8645833333333334, 0.8425, 0.8679166666666667, 0.86125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 350 \ttraining acc: [0.17208333 0.85166667 0.86875    0.87041667 0.87166667 0.87291667]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334, 0.7779166666666667, 0.7904166666666667, 0.84, 0.8120833333333334, 0.8470833333333333, 0.8454166666666667, 0.82875, 0.8070833333333334, 0.8283333333333334, 0.83375, 0.8108333333333333, 0.8358333333333333, 0.8333333333333334, 0.8166666666666667, 0.84625, 0.83, 0.84125, 0.8308333333333333, 0.8283333333333334, 0.8283333333333334, 0.8354166666666667, 0.83, 0.83875, 0.8625, 0.82125, 0.8195833333333333, 0.84, 0.8379166666666666, 0.8283333333333334, 0.8233333333333334, 0.85125, 0.8445833333333334, 0.8520833333333333, 0.8429166666666666, 0.81875, 0.8295833333333333, 0.84125, 0.8508333333333333, 0.84125, 0.84125, 0.8495833333333334, 0.8375, 0.8154166666666667, 0.8554166666666667, 0.8566666666666667, 0.8645833333333334, 0.8233333333333334, 0.86375, 0.8466666666666667, 0.8275, 0.8633333333333333, 0.8558333333333333, 0.8591666666666666, 0.8475, 0.8229166666666666, 0.84375, 0.84, 0.85375, 0.8529166666666667, 0.8491666666666666, 0.8870833333333333, 0.8495833333333334, 0.8233333333333334, 0.8491666666666666, 0.8708333333333333, 0.8591666666666666, 0.8541666666666666, 0.8458333333333333, 0.8679166666666667, 0.84375, 0.8579166666666667, 0.8625, 0.8408333333333333, 0.8816666666666667, 0.85625, 0.8483333333333334, 0.8220833333333334, 0.8595833333333334, 0.8729166666666667, 0.8554166666666667, 0.89125, 0.86125, 0.8554166666666667, 0.8575, 0.8904166666666666, 0.8595833333333334, 0.8233333333333334, 0.8558333333333333, 0.8554166666666667, 0.8825, 0.8633333333333333, 0.8808333333333334, 0.90125, 0.8820833333333333, 0.84875, 0.8708333333333333, 0.8645833333333334, 0.8425, 0.8679166666666667, 0.86125, 0.85, 0.8504166666666667, 0.8666666666666667, 0.8604166666666667, 0.8866666666666667, 0.8883333333333333, 0.8495833333333334, 0.8516666666666667, 0.8691666666666666, 0.87375, 0.8545833333333334, 0.8816666666666667, 0.8691666666666666, 0.8416666666666667, 0.8558333333333333, 0.8991666666666667, 0.8645833333333334, 0.85625, 0.85, 0.86375, 0.89375, 0.8758333333333334, 0.8720833333333333, 0.8566666666666667, 0.8633333333333333, 0.87625, 0.885, 0.89, 0.8670833333333333, 0.8320833333333333, 0.8833333333333333, 0.8633333333333333, 0.8945833333333333, 0.87375, 0.88625, 0.8754166666666666, 0.875, 0.88, 0.8933333333333333, 0.8845833333333334, 0.875, 0.8758333333333334, 0.8691666666666666, 0.8845833333333334, 0.8816666666666667, 0.8758333333333334, 0.86625, 0.8941666666666667, 0.9041666666666667, 0.8729166666666667]\n",
      "step: 400 \ttraining acc: [0.1625     0.89458333 0.91375    0.91375    0.91541667 0.91541667]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334, 0.7779166666666667, 0.7904166666666667, 0.84, 0.8120833333333334, 0.8470833333333333, 0.8454166666666667, 0.82875, 0.8070833333333334, 0.8283333333333334, 0.83375, 0.8108333333333333, 0.8358333333333333, 0.8333333333333334, 0.8166666666666667, 0.84625, 0.83, 0.84125, 0.8308333333333333, 0.8283333333333334, 0.8283333333333334, 0.8354166666666667, 0.83, 0.83875, 0.8625, 0.82125, 0.8195833333333333, 0.84, 0.8379166666666666, 0.8283333333333334, 0.8233333333333334, 0.85125, 0.8445833333333334, 0.8520833333333333, 0.8429166666666666, 0.81875, 0.8295833333333333, 0.84125, 0.8508333333333333, 0.84125, 0.84125, 0.8495833333333334, 0.8375, 0.8154166666666667, 0.8554166666666667, 0.8566666666666667, 0.8645833333333334, 0.8233333333333334, 0.86375, 0.8466666666666667, 0.8275, 0.8633333333333333, 0.8558333333333333, 0.8591666666666666, 0.8475, 0.8229166666666666, 0.84375, 0.84, 0.85375, 0.8529166666666667, 0.8491666666666666, 0.8870833333333333, 0.8495833333333334, 0.8233333333333334, 0.8491666666666666, 0.8708333333333333, 0.8591666666666666, 0.8541666666666666, 0.8458333333333333, 0.8679166666666667, 0.84375, 0.8579166666666667, 0.8625, 0.8408333333333333, 0.8816666666666667, 0.85625, 0.8483333333333334, 0.8220833333333334, 0.8595833333333334, 0.8729166666666667, 0.8554166666666667, 0.89125, 0.86125, 0.8554166666666667, 0.8575, 0.8904166666666666, 0.8595833333333334, 0.8233333333333334, 0.8558333333333333, 0.8554166666666667, 0.8825, 0.8633333333333333, 0.8808333333333334, 0.90125, 0.8820833333333333, 0.84875, 0.8708333333333333, 0.8645833333333334, 0.8425, 0.8679166666666667, 0.86125, 0.85, 0.8504166666666667, 0.8666666666666667, 0.8604166666666667, 0.8866666666666667, 0.8883333333333333, 0.8495833333333334, 0.8516666666666667, 0.8691666666666666, 0.87375, 0.8545833333333334, 0.8816666666666667, 0.8691666666666666, 0.8416666666666667, 0.8558333333333333, 0.8991666666666667, 0.8645833333333334, 0.85625, 0.85, 0.86375, 0.89375, 0.8758333333333334, 0.8720833333333333, 0.8566666666666667, 0.8633333333333333, 0.87625, 0.885, 0.89, 0.8670833333333333, 0.8320833333333333, 0.8833333333333333, 0.8633333333333333, 0.8945833333333333, 0.87375, 0.88625, 0.8754166666666666, 0.875, 0.88, 0.8933333333333333, 0.8845833333333334, 0.875, 0.8758333333333334, 0.8691666666666666, 0.8845833333333334, 0.8816666666666667, 0.8758333333333334, 0.86625, 0.8941666666666667, 0.9041666666666667, 0.8729166666666667, 0.9020833333333333, 0.8995833333333333, 0.8570833333333333, 0.8675, 0.8770833333333333, 0.8470833333333333, 0.8733333333333333, 0.8904166666666666, 0.9016666666666666, 0.8595833333333334, 0.8770833333333333, 0.8791666666666667, 0.8604166666666667, 0.8783333333333333, 0.8708333333333333, 0.8704166666666666, 0.8783333333333333, 0.8758333333333334, 0.8725, 0.8825, 0.8725, 0.8758333333333334, 0.8979166666666667, 0.8658333333333333, 0.9158333333333334, 0.8658333333333333, 0.89625, 0.8375, 0.91375, 0.8770833333333333, 0.855, 0.8920833333333333, 0.88375, 0.915, 0.88375, 0.8979166666666667, 0.8783333333333333, 0.8854166666666666, 0.86875, 0.9041666666666667, 0.89375, 0.89375, 0.875, 0.8441666666666666, 0.875, 0.8754166666666666, 0.8825, 0.87625, 0.8875, 0.9154166666666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 450 \ttraining acc: [0.20916667 0.87       0.89708333 0.89916667 0.89833333 0.89916667]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334, 0.7779166666666667, 0.7904166666666667, 0.84, 0.8120833333333334, 0.8470833333333333, 0.8454166666666667, 0.82875, 0.8070833333333334, 0.8283333333333334, 0.83375, 0.8108333333333333, 0.8358333333333333, 0.8333333333333334, 0.8166666666666667, 0.84625, 0.83, 0.84125, 0.8308333333333333, 0.8283333333333334, 0.8283333333333334, 0.8354166666666667, 0.83, 0.83875, 0.8625, 0.82125, 0.8195833333333333, 0.84, 0.8379166666666666, 0.8283333333333334, 0.8233333333333334, 0.85125, 0.8445833333333334, 0.8520833333333333, 0.8429166666666666, 0.81875, 0.8295833333333333, 0.84125, 0.8508333333333333, 0.84125, 0.84125, 0.8495833333333334, 0.8375, 0.8154166666666667, 0.8554166666666667, 0.8566666666666667, 0.8645833333333334, 0.8233333333333334, 0.86375, 0.8466666666666667, 0.8275, 0.8633333333333333, 0.8558333333333333, 0.8591666666666666, 0.8475, 0.8229166666666666, 0.84375, 0.84, 0.85375, 0.8529166666666667, 0.8491666666666666, 0.8870833333333333, 0.8495833333333334, 0.8233333333333334, 0.8491666666666666, 0.8708333333333333, 0.8591666666666666, 0.8541666666666666, 0.8458333333333333, 0.8679166666666667, 0.84375, 0.8579166666666667, 0.8625, 0.8408333333333333, 0.8816666666666667, 0.85625, 0.8483333333333334, 0.8220833333333334, 0.8595833333333334, 0.8729166666666667, 0.8554166666666667, 0.89125, 0.86125, 0.8554166666666667, 0.8575, 0.8904166666666666, 0.8595833333333334, 0.8233333333333334, 0.8558333333333333, 0.8554166666666667, 0.8825, 0.8633333333333333, 0.8808333333333334, 0.90125, 0.8820833333333333, 0.84875, 0.8708333333333333, 0.8645833333333334, 0.8425, 0.8679166666666667, 0.86125, 0.85, 0.8504166666666667, 0.8666666666666667, 0.8604166666666667, 0.8866666666666667, 0.8883333333333333, 0.8495833333333334, 0.8516666666666667, 0.8691666666666666, 0.87375, 0.8545833333333334, 0.8816666666666667, 0.8691666666666666, 0.8416666666666667, 0.8558333333333333, 0.8991666666666667, 0.8645833333333334, 0.85625, 0.85, 0.86375, 0.89375, 0.8758333333333334, 0.8720833333333333, 0.8566666666666667, 0.8633333333333333, 0.87625, 0.885, 0.89, 0.8670833333333333, 0.8320833333333333, 0.8833333333333333, 0.8633333333333333, 0.8945833333333333, 0.87375, 0.88625, 0.8754166666666666, 0.875, 0.88, 0.8933333333333333, 0.8845833333333334, 0.875, 0.8758333333333334, 0.8691666666666666, 0.8845833333333334, 0.8816666666666667, 0.8758333333333334, 0.86625, 0.8941666666666667, 0.9041666666666667, 0.8729166666666667, 0.9020833333333333, 0.8995833333333333, 0.8570833333333333, 0.8675, 0.8770833333333333, 0.8470833333333333, 0.8733333333333333, 0.8904166666666666, 0.9016666666666666, 0.8595833333333334, 0.8770833333333333, 0.8791666666666667, 0.8604166666666667, 0.8783333333333333, 0.8708333333333333, 0.8704166666666666, 0.8783333333333333, 0.8758333333333334, 0.8725, 0.8825, 0.8725, 0.8758333333333334, 0.8979166666666667, 0.8658333333333333, 0.9158333333333334, 0.8658333333333333, 0.89625, 0.8375, 0.91375, 0.8770833333333333, 0.855, 0.8920833333333333, 0.88375, 0.915, 0.88375, 0.8979166666666667, 0.8783333333333333, 0.8854166666666666, 0.86875, 0.9041666666666667, 0.89375, 0.89375, 0.875, 0.8441666666666666, 0.875, 0.8754166666666666, 0.8825, 0.87625, 0.8875, 0.9154166666666667, 0.8870833333333333, 0.8879166666666667, 0.8904166666666666, 0.8845833333333334, 0.8804166666666666, 0.8916666666666667, 0.8966666666666666, 0.9066666666666666, 0.8904166666666666, 0.8933333333333333, 0.8995833333333333, 0.8858333333333334, 0.86875, 0.8883333333333333, 0.91125, 0.8895833333333333, 0.8979166666666667, 0.8658333333333333, 0.8808333333333334, 0.8729166666666667, 0.9016666666666666, 0.8979166666666667, 0.8716666666666667, 0.8945833333333333, 0.8770833333333333, 0.8883333333333333, 0.8866666666666667, 0.8991666666666667, 0.8845833333333334, 0.87125, 0.9104166666666667, 0.8945833333333333, 0.9016666666666666, 0.89625, 0.8575, 0.8875, 0.9116666666666666, 0.8870833333333333, 0.885, 0.9016666666666666, 0.8945833333333333, 0.88125, 0.9158333333333334, 0.8891666666666667, 0.90125, 0.8770833333333333, 0.9075, 0.89, 0.8958333333333334, 0.8991666666666667]\n",
      "step: 500 \ttraining acc: [0.24458333 0.90041667 0.91291667 0.91541667 0.91541667 0.91541667]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334, 0.7779166666666667, 0.7904166666666667, 0.84, 0.8120833333333334, 0.8470833333333333, 0.8454166666666667, 0.82875, 0.8070833333333334, 0.8283333333333334, 0.83375, 0.8108333333333333, 0.8358333333333333, 0.8333333333333334, 0.8166666666666667, 0.84625, 0.83, 0.84125, 0.8308333333333333, 0.8283333333333334, 0.8283333333333334, 0.8354166666666667, 0.83, 0.83875, 0.8625, 0.82125, 0.8195833333333333, 0.84, 0.8379166666666666, 0.8283333333333334, 0.8233333333333334, 0.85125, 0.8445833333333334, 0.8520833333333333, 0.8429166666666666, 0.81875, 0.8295833333333333, 0.84125, 0.8508333333333333, 0.84125, 0.84125, 0.8495833333333334, 0.8375, 0.8154166666666667, 0.8554166666666667, 0.8566666666666667, 0.8645833333333334, 0.8233333333333334, 0.86375, 0.8466666666666667, 0.8275, 0.8633333333333333, 0.8558333333333333, 0.8591666666666666, 0.8475, 0.8229166666666666, 0.84375, 0.84, 0.85375, 0.8529166666666667, 0.8491666666666666, 0.8870833333333333, 0.8495833333333334, 0.8233333333333334, 0.8491666666666666, 0.8708333333333333, 0.8591666666666666, 0.8541666666666666, 0.8458333333333333, 0.8679166666666667, 0.84375, 0.8579166666666667, 0.8625, 0.8408333333333333, 0.8816666666666667, 0.85625, 0.8483333333333334, 0.8220833333333334, 0.8595833333333334, 0.8729166666666667, 0.8554166666666667, 0.89125, 0.86125, 0.8554166666666667, 0.8575, 0.8904166666666666, 0.8595833333333334, 0.8233333333333334, 0.8558333333333333, 0.8554166666666667, 0.8825, 0.8633333333333333, 0.8808333333333334, 0.90125, 0.8820833333333333, 0.84875, 0.8708333333333333, 0.8645833333333334, 0.8425, 0.8679166666666667, 0.86125, 0.85, 0.8504166666666667, 0.8666666666666667, 0.8604166666666667, 0.8866666666666667, 0.8883333333333333, 0.8495833333333334, 0.8516666666666667, 0.8691666666666666, 0.87375, 0.8545833333333334, 0.8816666666666667, 0.8691666666666666, 0.8416666666666667, 0.8558333333333333, 0.8991666666666667, 0.8645833333333334, 0.85625, 0.85, 0.86375, 0.89375, 0.8758333333333334, 0.8720833333333333, 0.8566666666666667, 0.8633333333333333, 0.87625, 0.885, 0.89, 0.8670833333333333, 0.8320833333333333, 0.8833333333333333, 0.8633333333333333, 0.8945833333333333, 0.87375, 0.88625, 0.8754166666666666, 0.875, 0.88, 0.8933333333333333, 0.8845833333333334, 0.875, 0.8758333333333334, 0.8691666666666666, 0.8845833333333334, 0.8816666666666667, 0.8758333333333334, 0.86625, 0.8941666666666667, 0.9041666666666667, 0.8729166666666667, 0.9020833333333333, 0.8995833333333333, 0.8570833333333333, 0.8675, 0.8770833333333333, 0.8470833333333333, 0.8733333333333333, 0.8904166666666666, 0.9016666666666666, 0.8595833333333334, 0.8770833333333333, 0.8791666666666667, 0.8604166666666667, 0.8783333333333333, 0.8708333333333333, 0.8704166666666666, 0.8783333333333333, 0.8758333333333334, 0.8725, 0.8825, 0.8725, 0.8758333333333334, 0.8979166666666667, 0.8658333333333333, 0.9158333333333334, 0.8658333333333333, 0.89625, 0.8375, 0.91375, 0.8770833333333333, 0.855, 0.8920833333333333, 0.88375, 0.915, 0.88375, 0.8979166666666667, 0.8783333333333333, 0.8854166666666666, 0.86875, 0.9041666666666667, 0.89375, 0.89375, 0.875, 0.8441666666666666, 0.875, 0.8754166666666666, 0.8825, 0.87625, 0.8875, 0.9154166666666667, 0.8870833333333333, 0.8879166666666667, 0.8904166666666666, 0.8845833333333334, 0.8804166666666666, 0.8916666666666667, 0.8966666666666666, 0.9066666666666666, 0.8904166666666666, 0.8933333333333333, 0.8995833333333333, 0.8858333333333334, 0.86875, 0.8883333333333333, 0.91125, 0.8895833333333333, 0.8979166666666667, 0.8658333333333333, 0.8808333333333334, 0.8729166666666667, 0.9016666666666666, 0.8979166666666667, 0.8716666666666667, 0.8945833333333333, 0.8770833333333333, 0.8883333333333333, 0.8866666666666667, 0.8991666666666667, 0.8845833333333334, 0.87125, 0.9104166666666667, 0.8945833333333333, 0.9016666666666666, 0.89625, 0.8575, 0.8875, 0.9116666666666666, 0.8870833333333333, 0.885, 0.9016666666666666, 0.8945833333333333, 0.88125, 0.9158333333333334, 0.8891666666666667, 0.90125, 0.8770833333333333, 0.9075, 0.89, 0.8958333333333334, 0.8991666666666667, 0.8779166666666667, 0.8858333333333334, 0.875, 0.8575, 0.8841666666666667, 0.8833333333333333, 0.91125, 0.9016666666666666, 0.8991666666666667, 0.8908333333333334, 0.9029166666666667, 0.9108333333333334, 0.8875, 0.8958333333333334, 0.8879166666666667, 0.9220833333333334, 0.8879166666666667, 0.8920833333333333, 0.89, 0.8904166666666666, 0.9170833333333334, 0.8941666666666667, 0.9095833333333333, 0.9220833333333334, 0.9145833333333333, 0.9045833333333333, 0.89875, 0.885, 0.9141666666666667, 0.8870833333333333, 0.8745833333333334, 0.8833333333333333, 0.8695833333333334, 0.8920833333333333, 0.8883333333333333, 0.8733333333333333, 0.8754166666666666, 0.8895833333333333, 0.90875, 0.8704166666666666, 0.8841666666666667, 0.9, 0.88875, 0.9075, 0.9083333333333333, 0.87625, 0.9075, 0.8920833333333333, 0.8954166666666666, 0.9154166666666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: [0.2001 0.8706 0.887  0.8896 0.891  0.8916 0.8926 0.8926 0.893  0.893\n",
      " 0.8936]\n",
      "test_accs_list: [0.4302, 0.8936]\n",
      "step: 550 \ttraining acc: [0.17791667 0.85208333 0.87916667 0.88583333 0.88625    0.88666667]\n",
      "val_accs: [0.3804166666666667, 0.40791666666666665, 0.4175, 0.4454166666666667, 0.39208333333333334, 0.43333333333333335, 0.4558333333333333, 0.4725, 0.47375, 0.43416666666666665, 0.4483333333333333, 0.44208333333333333, 0.50875, 0.4625, 0.48375, 0.4816666666666667, 0.48583333333333334, 0.5195833333333333, 0.50125, 0.52375, 0.5404166666666667, 0.5341666666666667, 0.5225, 0.53875, 0.5333333333333333, 0.5475, 0.5508333333333333, 0.5716666666666667, 0.5308333333333334, 0.5741666666666667, 0.5829166666666666, 0.54125, 0.5691666666666667, 0.59625, 0.59125, 0.56625, 0.5741666666666667, 0.5679166666666666, 0.5920833333333333, 0.5925, 0.5791666666666667, 0.59625, 0.6233333333333333, 0.59875, 0.60875, 0.5754166666666667, 0.5991666666666666, 0.5808333333333333, 0.60375, 0.6425, 0.61625, 0.6041666666666666, 0.6229166666666667, 0.6479166666666667, 0.6595833333333333, 0.6166666666666667, 0.6591666666666667, 0.6366666666666667, 0.6520833333333333, 0.6654166666666667, 0.6325, 0.61875, 0.6616666666666666, 0.6220833333333333, 0.67875, 0.685, 0.6566666666666666, 0.6983333333333334, 0.6966666666666667, 0.6708333333333333, 0.6741666666666667, 0.6829166666666666, 0.6895833333333333, 0.68, 0.65375, 0.67125, 0.6866666666666666, 0.6729166666666667, 0.6491666666666667, 0.68875, 0.64875, 0.645, 0.7091666666666666, 0.6775, 0.7029166666666666, 0.6616666666666666, 0.6958333333333333, 0.6825, 0.6716666666666666, 0.7108333333333333, 0.7025, 0.7020833333333333, 0.7075, 0.7270833333333333, 0.7083333333333334, 0.67875, 0.6879166666666666, 0.6925, 0.7016666666666667, 0.6575, 0.6708333333333333, 0.7170833333333333, 0.7216666666666667, 0.7229166666666667, 0.64875, 0.7104166666666667, 0.76375, 0.7308333333333333, 0.6983333333333334, 0.7304166666666667, 0.7108333333333333, 0.71375, 0.7145833333333333, 0.7370833333333333, 0.73125, 0.7325, 0.7325, 0.7408333333333333, 0.7341666666666666, 0.7283333333333334, 0.7254166666666667, 0.6866666666666666, 0.7375, 0.7154166666666667, 0.7558333333333334, 0.74, 0.7270833333333333, 0.74875, 0.7591666666666667, 0.76875, 0.77625, 0.7341666666666666, 0.75375, 0.72375, 0.7566666666666667, 0.77125, 0.7558333333333334, 0.75, 0.7470833333333333, 0.7458333333333333, 0.7525, 0.7545833333333334, 0.7858333333333334, 0.7429166666666667, 0.755, 0.7691666666666667, 0.7920833333333334, 0.7904166666666667, 0.7625, 0.7958333333333333, 0.7641666666666667, 0.7741666666666667, 0.765, 0.7358333333333333, 0.7425, 0.7808333333333334, 0.7758333333333334, 0.7558333333333334, 0.7925, 0.785, 0.7954166666666667, 0.7816666666666666, 0.79, 0.7633333333333333, 0.7941666666666667, 0.815, 0.8454166666666667, 0.8145833333333333, 0.78625, 0.7991666666666667, 0.7525, 0.8091666666666667, 0.7945833333333333, 0.7754166666666666, 0.785, 0.7975, 0.8, 0.79375, 0.8329166666666666, 0.7770833333333333, 0.7995833333333333, 0.8104166666666667, 0.8041666666666667, 0.78, 0.78125, 0.8083333333333333, 0.8170833333333334, 0.7908333333333334, 0.7945833333333333, 0.8179166666666666, 0.8304166666666667, 0.8108333333333333, 0.8270833333333333, 0.8558333333333333, 0.7916666666666666, 0.8383333333333334, 0.81375, 0.8029166666666666, 0.835, 0.8279166666666666, 0.8333333333333334, 0.7779166666666667, 0.7904166666666667, 0.84, 0.8120833333333334, 0.8470833333333333, 0.8454166666666667, 0.82875, 0.8070833333333334, 0.8283333333333334, 0.83375, 0.8108333333333333, 0.8358333333333333, 0.8333333333333334, 0.8166666666666667, 0.84625, 0.83, 0.84125, 0.8308333333333333, 0.8283333333333334, 0.8283333333333334, 0.8354166666666667, 0.83, 0.83875, 0.8625, 0.82125, 0.8195833333333333, 0.84, 0.8379166666666666, 0.8283333333333334, 0.8233333333333334, 0.85125, 0.8445833333333334, 0.8520833333333333, 0.8429166666666666, 0.81875, 0.8295833333333333, 0.84125, 0.8508333333333333, 0.84125, 0.84125, 0.8495833333333334, 0.8375, 0.8154166666666667, 0.8554166666666667, 0.8566666666666667, 0.8645833333333334, 0.8233333333333334, 0.86375, 0.8466666666666667, 0.8275, 0.8633333333333333, 0.8558333333333333, 0.8591666666666666, 0.8475, 0.8229166666666666, 0.84375, 0.84, 0.85375, 0.8529166666666667, 0.8491666666666666, 0.8870833333333333, 0.8495833333333334, 0.8233333333333334, 0.8491666666666666, 0.8708333333333333, 0.8591666666666666, 0.8541666666666666, 0.8458333333333333, 0.8679166666666667, 0.84375, 0.8579166666666667, 0.8625, 0.8408333333333333, 0.8816666666666667, 0.85625, 0.8483333333333334, 0.8220833333333334, 0.8595833333333334, 0.8729166666666667, 0.8554166666666667, 0.89125, 0.86125, 0.8554166666666667, 0.8575, 0.8904166666666666, 0.8595833333333334, 0.8233333333333334, 0.8558333333333333, 0.8554166666666667, 0.8825, 0.8633333333333333, 0.8808333333333334, 0.90125, 0.8820833333333333, 0.84875, 0.8708333333333333, 0.8645833333333334, 0.8425, 0.8679166666666667, 0.86125, 0.85, 0.8504166666666667, 0.8666666666666667, 0.8604166666666667, 0.8866666666666667, 0.8883333333333333, 0.8495833333333334, 0.8516666666666667, 0.8691666666666666, 0.87375, 0.8545833333333334, 0.8816666666666667, 0.8691666666666666, 0.8416666666666667, 0.8558333333333333, 0.8991666666666667, 0.8645833333333334, 0.85625, 0.85, 0.86375, 0.89375, 0.8758333333333334, 0.8720833333333333, 0.8566666666666667, 0.8633333333333333, 0.87625, 0.885, 0.89, 0.8670833333333333, 0.8320833333333333, 0.8833333333333333, 0.8633333333333333, 0.8945833333333333, 0.87375, 0.88625, 0.8754166666666666, 0.875, 0.88, 0.8933333333333333, 0.8845833333333334, 0.875, 0.8758333333333334, 0.8691666666666666, 0.8845833333333334, 0.8816666666666667, 0.8758333333333334, 0.86625, 0.8941666666666667, 0.9041666666666667, 0.8729166666666667, 0.9020833333333333, 0.8995833333333333, 0.8570833333333333, 0.8675, 0.8770833333333333, 0.8470833333333333, 0.8733333333333333, 0.8904166666666666, 0.9016666666666666, 0.8595833333333334, 0.8770833333333333, 0.8791666666666667, 0.8604166666666667, 0.8783333333333333, 0.8708333333333333, 0.8704166666666666, 0.8783333333333333, 0.8758333333333334, 0.8725, 0.8825, 0.8725, 0.8758333333333334, 0.8979166666666667, 0.8658333333333333, 0.9158333333333334, 0.8658333333333333, 0.89625, 0.8375, 0.91375, 0.8770833333333333, 0.855, 0.8920833333333333, 0.88375, 0.915, 0.88375, 0.8979166666666667, 0.8783333333333333, 0.8854166666666666, 0.86875, 0.9041666666666667, 0.89375, 0.89375, 0.875, 0.8441666666666666, 0.875, 0.8754166666666666, 0.8825, 0.87625, 0.8875, 0.9154166666666667, 0.8870833333333333, 0.8879166666666667, 0.8904166666666666, 0.8845833333333334, 0.8804166666666666, 0.8916666666666667, 0.8966666666666666, 0.9066666666666666, 0.8904166666666666, 0.8933333333333333, 0.8995833333333333, 0.8858333333333334, 0.86875, 0.8883333333333333, 0.91125, 0.8895833333333333, 0.8979166666666667, 0.8658333333333333, 0.8808333333333334, 0.8729166666666667, 0.9016666666666666, 0.8979166666666667, 0.8716666666666667, 0.8945833333333333, 0.8770833333333333, 0.8883333333333333, 0.8866666666666667, 0.8991666666666667, 0.8845833333333334, 0.87125, 0.9104166666666667, 0.8945833333333333, 0.9016666666666666, 0.89625, 0.8575, 0.8875, 0.9116666666666666, 0.8870833333333333, 0.885, 0.9016666666666666, 0.8945833333333333, 0.88125, 0.9158333333333334, 0.8891666666666667, 0.90125, 0.8770833333333333, 0.9075, 0.89, 0.8958333333333334, 0.8991666666666667, 0.8779166666666667, 0.8858333333333334, 0.875, 0.8575, 0.8841666666666667, 0.8833333333333333, 0.91125, 0.9016666666666666, 0.8991666666666667, 0.8908333333333334, 0.9029166666666667, 0.9108333333333334, 0.8875, 0.8958333333333334, 0.8879166666666667, 0.9220833333333334, 0.8879166666666667, 0.8920833333333333, 0.89, 0.8904166666666666, 0.9170833333333334, 0.8941666666666667, 0.9095833333333333, 0.9220833333333334, 0.9145833333333333, 0.9045833333333333, 0.89875, 0.885, 0.9141666666666667, 0.8870833333333333, 0.8745833333333334, 0.8833333333333333, 0.8695833333333334, 0.8920833333333333, 0.8883333333333333, 0.8733333333333333, 0.8754166666666666, 0.8895833333333333, 0.90875, 0.8704166666666666, 0.8841666666666667, 0.9, 0.88875, 0.9075, 0.9083333333333333, 0.87625, 0.9075, 0.8920833333333333, 0.8954166666666666, 0.9154166666666667, 0.9116666666666666, 0.9016666666666666, 0.86375, 0.8866666666666667, 0.9120833333333334, 0.9016666666666666, 0.9025, 0.8816666666666667, 0.88875, 0.9004166666666666, 0.9091666666666667, 0.8941666666666667, 0.9083333333333333, 0.9033333333333333, 0.9066666666666666, 0.90125, 0.9029166666666667, 0.8845833333333334, 0.9183333333333333, 0.8841666666666667, 0.8829166666666667, 0.9033333333333333, 0.92875, 0.89625, 0.9045833333333333, 0.8741666666666666, 0.8916666666666667, 0.8983333333333333, 0.9029166666666667, 0.9225, 0.9029166666666667, 0.8891666666666667, 0.9054166666666666, 0.8916666666666667, 0.88625, 0.92375, 0.8983333333333333, 0.9275, 0.88875, 0.91375, 0.905, 0.9120833333333334, 0.9054166666666666, 0.9129166666666667, 0.9120833333333334, 0.9, 0.92625, 0.9091666666666667, 0.9075, 0.8866666666666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/matthewho/Photonic_computing/MAML-Pytorch/omniglot_train.py\", line 106, in <module>\r\n",
      "    main(args)\r\n",
      "  File \"/Users/matthewho/Photonic_computing/MAML-Pytorch/omniglot_train.py\", line 61, in main\r\n",
      "    accs = maml(x_spt, y_spt, x_qry, y_qry)\r\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n",
      "    return forward_call(*args, **kwargs)\r\n",
      "  File \"/Users/matthewho/Photonic_computing/MAML-Pytorch/meta.py\", line 103, in forward\r\n",
      "    logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\r\n",
      "  File \"/Users/matthewho/Photonic_computing/photonics_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n",
      "    return forward_call(*args, **kwargs)\r\n",
      "  File \"/Users/matthewho/Photonic_computing/MAML-Pytorch/learner.py\", line 144, in forward\r\n",
      "    x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python omniglot_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c65a59b-a7cc-4546-a2cb-df65640cffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('omniglot_maml.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd5cd9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2390625 , 0.975     , 0.984375  , 0.984375  , 0.98515625,\n",
       "       0.98515625])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_spt, y_spt, x_qry, y_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe8cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
